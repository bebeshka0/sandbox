{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json \n",
    "import pandas as pd \n",
    "import nltk \n",
    "import numpy as np \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'reviewerID': 'AZPWAXJG9OJXV',\n",
       "  'asin': '5555991584',\n",
       "  'reviewerName': 'bethtexas',\n",
       "  'helpful': [0, 0],\n",
       "  'reviewText': \"A clasically-styled and introverted album, Memory of Trees is a masterpiece of subtlety.  Many of the songs have an endearing shyness to them - soft piano and a lovely, quiet voice.  But within every introvert is an inferno, and Enya lets that fire explode on a couple of songs that absolutely burst with an expected raw power.If you've never heard Enya before, you might want to start with one of her more popularized works, like Watermark, just to play it safe.  But if you're already a fan, then your collection is not complete without this beautiful work of musical art.\",\n",
       "  'overall': 5.0,\n",
       "  'summary': 'Enya at her most elegant',\n",
       "  'unixReviewTime': 991526400,\n",
       "  'reviewTime': '06 3, 2001'},\n",
       " {'reviewerID': 'A38IRL0X2T4DPF',\n",
       "  'asin': '5555991584',\n",
       "  'reviewerName': 'bob turnley',\n",
       "  'helpful': [2, 2],\n",
       "  'reviewText': \"I never thought Enya would reach the sublime heights of Evacuee or Marble Halls from 'Shepherd Moons.' 'The Celts, Watermark and Day...' were all pleasant and admirable throughout, but are less ambitious both lyrically and musically. But Hope Has a Place from 'Memory...' reaches those heights and beyond. It is Enya at her most inspirational and comforting. I'm actually glad that this song didn't get overexposed the way Only Time did. It makes it that much more special to all who own this album.\",\n",
       "  'overall': 5.0,\n",
       "  'summary': 'The best so far',\n",
       "  'unixReviewTime': 1058140800,\n",
       "  'reviewTime': '07 14, 2003'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = 10000\n",
    "data = [json.loads(f) for f in open(\"Digital_Music_5.json\", 'r')]\n",
    "type(data) \n",
    "data[1:3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64706, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "overall\n",
       "5.0    0.549872\n",
       "4.0    0.255556\n",
       "3.0    0.104921\n",
       "2.0    0.046518\n",
       "1.0    0.043134\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.shape)\n",
    "df.overall.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.75])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Weights'] = np.where(df['overall'] < 5, .75, .25)\n",
    "df['Weights'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(n = sample_size, random_state = 10, weights = 'Weights')\n",
    "\n",
    "sample = sample.dropna(how = 'any', subset = ['reviewText', 'overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>Weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50925</th>\n",
       "      <td>A1QFQ9SQZVGUK4</td>\n",
       "      <td>B000BGR18W</td>\n",
       "      <td>Brandon L. Harlow</td>\n",
       "      <td>[4, 23]</td>\n",
       "      <td>Oh why even bother wasting the energy to type....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>well</td>\n",
       "      <td>1133481600</td>\n",
       "      <td>12 2, 2005</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>A2JAKHFYI88ZX0</td>\n",
       "      <td>B000000ORH</td>\n",
       "      <td>Jake Z \"holden84\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>HEART OF STONE is a fine Cher album, although ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80s Cher</td>\n",
       "      <td>1086480000</td>\n",
       "      <td>06 6, 2004</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43170</th>\n",
       "      <td>A3IPO6P4LHZ0NS</td>\n",
       "      <td>B0000AKCLJ</td>\n",
       "      <td>Joe</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>I just finished listening to this c.d. and I h...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>RAVE ON!!!!!!!!</td>\n",
       "      <td>1067212800</td>\n",
       "      <td>10 27, 2003</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49625</th>\n",
       "      <td>ANBT9T1QKC662</td>\n",
       "      <td>B0009ML2BU</td>\n",
       "      <td>M. Manzino</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>I really love this CD! it is one of the most d...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A very different offering</td>\n",
       "      <td>1156982400</td>\n",
       "      <td>08 31, 2006</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35077</th>\n",
       "      <td>AKFDAV8I4FLUJ</td>\n",
       "      <td>B00005IBYZ</td>\n",
       "      <td>\"jcino\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>After a long time of success they broke up for...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Punk at it's best and it's saving rock and roll</td>\n",
       "      <td>994982400</td>\n",
       "      <td>07 13, 2001</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           reviewerID        asin       reviewerName  helpful  \\\n",
       "50925  A1QFQ9SQZVGUK4  B000BGR18W  Brandon L. Harlow  [4, 23]   \n",
       "1566   A2JAKHFYI88ZX0  B000000ORH  Jake Z \"holden84\"   [0, 0]   \n",
       "43170  A3IPO6P4LHZ0NS  B0000AKCLJ                Joe   [0, 1]   \n",
       "49625   ANBT9T1QKC662  B0009ML2BU         M. Manzino   [2, 2]   \n",
       "35077   AKFDAV8I4FLUJ  B00005IBYZ            \"jcino\"   [0, 0]   \n",
       "\n",
       "                                              reviewText  overall  \\\n",
       "50925  Oh why even bother wasting the energy to type....      1.0   \n",
       "1566   HEART OF STONE is a fine Cher album, although ...      4.0   \n",
       "43170  I just finished listening to this c.d. and I h...      4.0   \n",
       "49625  I really love this CD! it is one of the most d...      5.0   \n",
       "35077  After a long time of success they broke up for...      5.0   \n",
       "\n",
       "                                               summary  unixReviewTime  \\\n",
       "50925                                             well      1133481600   \n",
       "1566                                          80s Cher      1086480000   \n",
       "43170                                  RAVE ON!!!!!!!!      1067212800   \n",
       "49625                        A very different offering      1156982400   \n",
       "35077  Punk at it's best and it's saving rock and roll       994982400   \n",
       "\n",
       "        reviewTime  Weights  \n",
       "50925   12 2, 2005     0.75  \n",
       "1566    06 6, 2004     0.75  \n",
       "43170  10 27, 2003     0.75  \n",
       "49625  08 31, 2006     0.25  \n",
       "35077  07 13, 2001     0.25  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewerID         object\n",
       "asin               object\n",
       "reviewerName       object\n",
       "helpful            object\n",
       "reviewText         object\n",
       "overall           float64\n",
       "summary            object\n",
       "unixReviewTime      int64\n",
       "reviewTime         object\n",
       "Weights           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_18424\\2094404136.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    text = re.sub(r\"&[a-z]+;\", \" \", text)\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "sample['cleaned_text'] = sample['reviewText'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sample['tokenized_text'] = [\n",
    "    [token.text for token in doc] for doc in nlp.pipe(sample['cleaned_text'], batch_size=50, disable=[\"ner\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['filtered_text'] = sample['tokenized_text'].apply(\n",
    "    lambda tokens: [token for token in tokens if token.lower() not in nlp.Defaults.stop_words]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['lemmatized_text'] = sample['filtered_text'].apply(\n",
    "    lambda tokens: [token.lemma_ for token in nlp(' '.join(tokens))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accessible</th>\n",
       "      <th>accompany</th>\n",
       "      <th>achieve</th>\n",
       "      <th>acoustic</th>\n",
       "      <th>acoustic guitar</th>\n",
       "      <th>act</th>\n",
       "      <th>...</th>\n",
       "      <th>year later</th>\n",
       "      <th>year old</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>yo</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>young buck</th>\n",
       "      <th>youth</th>\n",
       "      <th>zeppelin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ability  able  absolute  absolutely  accessible  accompany  achieve  \\\n",
       "0      0.0   0.0       0.0         0.0         0.0        0.0      0.0   \n",
       "1      0.0   0.0       0.0         0.0         0.0        0.0      0.0   \n",
       "2      0.0   0.0       0.0         0.0         0.0        0.0      0.0   \n",
       "3      0.0   0.0       0.0         0.0         0.0        0.0      0.0   \n",
       "4      0.0   0.0       0.0         0.0         0.0        0.0      0.0   \n",
       "\n",
       "   acoustic  acoustic guitar  act  ...  year later  year old  yellow  \\\n",
       "0       0.0              0.0  0.0  ...         0.0       0.0     0.0   \n",
       "1       0.0              0.0  0.0  ...         0.0       0.0     0.0   \n",
       "2       0.0              0.0  0.0  ...         0.0       0.0     0.0   \n",
       "3       0.0              0.0  0.0  ...         0.0       0.0     0.0   \n",
       "4       0.0              0.0  0.0  ...         0.0       0.0     0.0   \n",
       "\n",
       "        yes   yo  york  young  young buck  youth  zeppelin  \n",
       "0  0.000000  0.0   0.0    0.0         0.0    0.0       0.0  \n",
       "1  0.000000  0.0   0.0    0.0         0.0    0.0       0.0  \n",
       "2  0.152947  0.0   0.0    0.0         0.0    0.0       0.0  \n",
       "3  0.163310  0.0   0.0    0.0         0.0    0.0       0.0  \n",
       "4  0.000000  0.0   0.0    0.0         0.0    0.0       0.0  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sample['lemmatized_text_str'] = sample['lemmatized_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=2000,\n",
    "    max_df=0.75,\n",
    "    min_df=1,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sample['lemmatized_text_str'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(tfidf_df.shape)\n",
    "tfidf_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50925</th>\n",
       "      <td>0.077830</td>\n",
       "      <td>-0.164135</td>\n",
       "      <td>0.106710</td>\n",
       "      <td>0.172955</td>\n",
       "      <td>0.120581</td>\n",
       "      <td>-0.710298</td>\n",
       "      <td>0.705848</td>\n",
       "      <td>0.736694</td>\n",
       "      <td>-0.295055</td>\n",
       "      <td>-0.485439</td>\n",
       "      <td>...</td>\n",
       "      <td>1.007800</td>\n",
       "      <td>-1.208359</td>\n",
       "      <td>0.092865</td>\n",
       "      <td>0.032358</td>\n",
       "      <td>1.106977</td>\n",
       "      <td>-0.464917</td>\n",
       "      <td>-0.408378</td>\n",
       "      <td>-0.625039</td>\n",
       "      <td>0.677953</td>\n",
       "      <td>-0.316360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>-0.355893</td>\n",
       "      <td>0.406210</td>\n",
       "      <td>0.241713</td>\n",
       "      <td>-0.281474</td>\n",
       "      <td>0.151394</td>\n",
       "      <td>0.079942</td>\n",
       "      <td>0.374164</td>\n",
       "      <td>0.784364</td>\n",
       "      <td>-0.418378</td>\n",
       "      <td>-0.394573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852004</td>\n",
       "      <td>-0.443970</td>\n",
       "      <td>0.027705</td>\n",
       "      <td>0.268810</td>\n",
       "      <td>0.452391</td>\n",
       "      <td>0.057623</td>\n",
       "      <td>-0.428250</td>\n",
       "      <td>-0.655849</td>\n",
       "      <td>0.312311</td>\n",
       "      <td>0.241001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43170</th>\n",
       "      <td>-0.079538</td>\n",
       "      <td>-0.013949</td>\n",
       "      <td>0.053334</td>\n",
       "      <td>-0.435483</td>\n",
       "      <td>0.016046</td>\n",
       "      <td>-0.276736</td>\n",
       "      <td>0.381305</td>\n",
       "      <td>0.760890</td>\n",
       "      <td>-0.412833</td>\n",
       "      <td>-0.455056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714164</td>\n",
       "      <td>-0.142776</td>\n",
       "      <td>0.280622</td>\n",
       "      <td>0.280068</td>\n",
       "      <td>0.690794</td>\n",
       "      <td>-0.209838</td>\n",
       "      <td>-0.233054</td>\n",
       "      <td>-0.595692</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>0.117975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49625</th>\n",
       "      <td>-0.122942</td>\n",
       "      <td>0.196906</td>\n",
       "      <td>0.087276</td>\n",
       "      <td>-0.366814</td>\n",
       "      <td>0.118666</td>\n",
       "      <td>-0.007325</td>\n",
       "      <td>0.415579</td>\n",
       "      <td>0.819426</td>\n",
       "      <td>-0.343563</td>\n",
       "      <td>-0.209667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704753</td>\n",
       "      <td>-0.190246</td>\n",
       "      <td>0.056054</td>\n",
       "      <td>0.214398</td>\n",
       "      <td>0.599333</td>\n",
       "      <td>-0.173858</td>\n",
       "      <td>-0.257384</td>\n",
       "      <td>-0.502007</td>\n",
       "      <td>0.110120</td>\n",
       "      <td>0.205686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35077</th>\n",
       "      <td>-0.286668</td>\n",
       "      <td>0.096776</td>\n",
       "      <td>0.331476</td>\n",
       "      <td>-0.616008</td>\n",
       "      <td>-0.350688</td>\n",
       "      <td>-0.116671</td>\n",
       "      <td>0.352164</td>\n",
       "      <td>0.660657</td>\n",
       "      <td>-0.303597</td>\n",
       "      <td>-0.443169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998863</td>\n",
       "      <td>-0.361392</td>\n",
       "      <td>0.171863</td>\n",
       "      <td>0.248618</td>\n",
       "      <td>0.290167</td>\n",
       "      <td>-0.544364</td>\n",
       "      <td>-0.271845</td>\n",
       "      <td>-0.779230</td>\n",
       "      <td>0.213398</td>\n",
       "      <td>0.265254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "50925  0.077830 -0.164135  0.106710  0.172955  0.120581 -0.710298  0.705848   \n",
       "1566  -0.355893  0.406210  0.241713 -0.281474  0.151394  0.079942  0.374164   \n",
       "43170 -0.079538 -0.013949  0.053334 -0.435483  0.016046 -0.276736  0.381305   \n",
       "49625 -0.122942  0.196906  0.087276 -0.366814  0.118666 -0.007325  0.415579   \n",
       "35077 -0.286668  0.096776  0.331476 -0.616008 -0.350688 -0.116671  0.352164   \n",
       "\n",
       "             7         8         9   ...        40        41        42  \\\n",
       "50925  0.736694 -0.295055 -0.485439  ...  1.007800 -1.208359  0.092865   \n",
       "1566   0.784364 -0.418378 -0.394573  ...  0.852004 -0.443970  0.027705   \n",
       "43170  0.760890 -0.412833 -0.455056  ...  0.714164 -0.142776  0.280622   \n",
       "49625  0.819426 -0.343563 -0.209667  ...  0.704753 -0.190246  0.056054   \n",
       "35077  0.660657 -0.303597 -0.443169  ...  0.998863 -0.361392  0.171863   \n",
       "\n",
       "             43        44        45        46        47        48        49  \n",
       "50925  0.032358  1.106977 -0.464917 -0.408378 -0.625039  0.677953 -0.316360  \n",
       "1566   0.268810  0.452391  0.057623 -0.428250 -0.655849  0.312311  0.241001  \n",
       "43170  0.280068  0.690794 -0.209838 -0.233054 -0.595692  0.104962  0.117975  \n",
       "49625  0.214398  0.599333 -0.173858 -0.257384 -0.502007  0.110120  0.205686  \n",
       "35077  0.248618  0.290167 -0.544364 -0.271845 -0.779230  0.213398  0.265254  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=sample['lemmatized_text'],\n",
    "    vector_size=50,  \n",
    "    window=7,         \n",
    "    min_count=5,      \n",
    "    workers=4         \n",
    ")\n",
    "\n",
    "def vectorize_text(text, model):\n",
    "    vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "    \n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "sample['word2vec_vector'] = sample['lemmatized_text'].apply(lambda x: vectorize_text(x, word2vec_model))\n",
    "\n",
    "word2vec_df = pd.DataFrame(sample['word2vec_vector'].to_list(), index=sample.index)\n",
    "\n",
    "print(word2vec_df.shape)\n",
    "word2vec_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('tfidf', TfidfVectorizer()),\n",
    "#     ('clf', LogisticRegression(solver='saga', max_iter=500))\n",
    "# ])\n",
    "\n",
    "# param_grid = {\n",
    "#     'tfidf__max_features': [500, 1000, 2000],       \n",
    "#     'tfidf__ngram_range': [(1, 1), (1, 2)],         \n",
    "#     'tfidf__max_df': [0.75, 0.85, 1.0],              \n",
    "#     'tfidf__min_df': [1, 2, 5],                    \n",
    "#     'clf__C': [0.1, 1, 10]                         \n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')  \n",
    "# grid_search.fit(sample['lemmatized_text_str'], sample['overall'])\n",
    "\n",
    "# print(\"best param:\", grid_search.best_params_)\n",
    "# print(\"best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import numpy as np\n",
    "\n",
    "# def vectorize_text(text, model):\n",
    "#     vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "#     if vectors:\n",
    "#         return np.mean(vectors, axis=0)\n",
    "#     else:\n",
    "#         return np.zeros(model.vector_size)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     sample['lemmatized_text'], sample['overall'], test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# best_score = 0\n",
    "# best_params = {}\n",
    "\n",
    "# for vector_size in [50, 100, 200]:\n",
    "#     for window in [3, 5, 7]:\n",
    "#         for min_count in [1, 2, 5]:\n",
    "#             print(f\"param: vector_size={vector_size}, window={window}, min_count={min_count}\")\n",
    "            \n",
    "#             word2vec_model = Word2Vec(\n",
    "#                 sentences=X_train,\n",
    "#                 vector_size=vector_size,\n",
    "#                 window=window,\n",
    "#                 min_count=min_count,\n",
    "#                 workers=4\n",
    "#             )\n",
    "            \n",
    "#             X_train_vectors = np.array([vectorize_text(text, word2vec_model) for text in X_train])\n",
    "#             X_test_vectors = np.array([vectorize_text(text, word2vec_model) for text in X_test])\n",
    "            \n",
    "#             clf = RandomForestClassifier()\n",
    "#             clf.fit(X_train_vectors, y_train)\n",
    "#             y_pred = clf.predict(X_test_vectors)\n",
    "\n",
    "#             score = accuracy_score(y_test, y_pred)\n",
    "#             print(f\"score: {score}\")\n",
    "            \n",
    "#             if score > best_score:\n",
    "#                 best_score = score\n",
    "#                 best_params = {'vector_size': vector_size, 'window': window, 'min_count': min_count}\n",
    "\n",
    "# print(\"best param:\", best_params)\n",
    "# print(\"best score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     sample['lemmatized_text_str'], sample['overall'], test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(\n",
    "#     max_features=2000,\n",
    "#     ngram_range=(1, 2),\n",
    "#     max_df=0.75,\n",
    "#     min_df=1\n",
    "# )\n",
    "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# logistic_clf = LogisticRegression(C=1, solver='saga', max_iter=500)\n",
    "# logistic_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# y_pred_tfidf = logistic_clf.predict(X_test_tfidf)\n",
    "# accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "\n",
    "# print(\"ac lr tfidf:\", accuracy_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# import numpy as np\n",
    "\n",
    "# word2vec_model = Word2Vec(\n",
    "#     sentences=sample['lemmatized_text'],\n",
    "#     vector_size=50,\n",
    "#     window=7,\n",
    "#     min_count=5\n",
    "# )\n",
    "\n",
    "# def vectorize_text(text, model):\n",
    "#     vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "#     return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# X_train_word2vec = np.array([vectorize_text(text, word2vec_model) for text in X_train])\n",
    "# X_test_word2vec = np.array([vectorize_text(text, word2vec_model) for text in X_test])\n",
    "\n",
    "# random_forest_clf = RandomForestClassifier()\n",
    "# random_forest_clf.fit(X_train_word2vec, y_train)\n",
    "\n",
    "# y_pred_word2vec = random_forest_clf.predict(X_test_word2vec)\n",
    "# accuracy_word2vec = accuracy_score(y_test, y_pred_word2vec)\n",
    "\n",
    "# print(\"ac rf w2v:\", accuracy_word2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import StackingClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "# from gensim.models import Word2Vec\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     sample['lemmatized_text_str'], sample['overall'], test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(\n",
    "#     max_features=2000,\n",
    "#     ngram_range=(1, 2),\n",
    "#     max_df=0.75,\n",
    "#     min_df=1\n",
    "# )\n",
    "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# logistic_clf = LogisticRegression(C=1, solver='saga', max_iter=500)\n",
    "\n",
    "# word2vec_model = Word2Vec(\n",
    "#     sentences=sample['lemmatized_text'],\n",
    "#     vector_size=50,\n",
    "#     window=7,\n",
    "#     min_count=5\n",
    "# )\n",
    "\n",
    "# def vectorize_text(text, model):\n",
    "#     vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "#     return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# X_train_word2vec = np.array([vectorize_text(text, word2vec_model) for text in X_train])\n",
    "# X_test_word2vec = np.array([vectorize_text(text, word2vec_model) for text in X_test])\n",
    "\n",
    "# random_forest_clf = RandomForestClassifier()\n",
    "\n",
    "# stacking_clf = StackingClassifier(\n",
    "#     estimators=[\n",
    "#         ('logistic', logistic_clf),\n",
    "#         ('random_forest', random_forest_clf)\n",
    "#     ],\n",
    "#     final_estimator=LogisticRegression()\n",
    "# )\n",
    "\n",
    "# stacking_clf.fit(np.hstack([X_train_tfidf.toarray(), X_train_word2vec]), y_train)\n",
    "\n",
    "# y_pred_stacking = stacking_clf.predict(np.hstack([X_test_tfidf.toarray(), X_test_word2vec]))\n",
    "# accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
    "\n",
    "# print(\"ac stack:\", accuracy_stacking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
>>>>>>> 1a47c36903e11a09f21bb481549c6cb8ccbae0f6
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# import numpy as np\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     sample['lemmatized_text_str'], sample['overall'], test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(\n",
    "#     max_features=2000,\n",
    "#     ngram_range=(1, 2),\n",
    "#     max_df=0.75,\n",
    "#     min_df=1\n",
    "# )\n",
    "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# word2vec_model = Word2Vec(\n",
    "#     sentences=sample['lemmatized_text'],\n",
    "#     vector_size=50,\n",
    "#     window=7,\n",
    "#     min_count=5\n",
    "# )\n",
    "\n",
    "# def vectorize_text(text, model):\n",
    "#     vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "#     return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# X_train_word2vec = np.array([vectorize_text(text, word2vec_model) for text in X_train])\n",
    "# X_test_word2vec = np.array([vectorize_text(text, word2vec_model) for text in X_test])\n",
    "\n",
    "# X_train_combined = np.hstack([X_train_tfidf.toarray(), X_train_word2vec])\n",
    "# X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_word2vec])\n",
    "\n",
    "# regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# regressor.fit(X_train_combined, y_train)\n",
    "\n",
    "# y_pred = regressor.predict(X_test_combined)\n",
    "\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(\"MAE:\", mae)\n",
    "# print(\"MSE:\", mse)\n",
    "\n",
    "# y_pred_rounded = np.round(y_pred)\n",
    "# accuracy = (y_pred_rounded == y_test).mean()\n",
    "# print(\"ac:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
>>>>>>> 1a47c36903e11a09f21bb481549c6cb8ccbae0f6
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# import numpy as np\n",
    "\n",
    "# X_train_small, _, y_train_small, _ = train_test_split(X_train_combined, y_train, test_size=0.8, random_state=42)\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 150, 200],\n",
    "#     'max_depth': [10, 20, None],\n",
    "#     'min_samples_split': [2, 5],\n",
    "#     'min_samples_leaf': [1, 2]\n",
    "# }\n",
    "\n",
    "# rf_regressor = RandomForestRegressor(random_state=42)\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=rf_regressor,\n",
    "#     param_distributions=param_grid,\n",
    "#     n_iter=10, \n",
    "#     scoring='neg_mean_absolute_error',\n",
    "#     cv=3,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# random_search.fit(X_train_small, y_train_small)\n",
    "# print(\"best param:\", random_search.best_params_)\n",
    "# print(\"mae:\", -random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
>>>>>>> 1a47c36903e11a09f21bb481549c6cb8ccbae0f6
   "source": [
    "\n",
    "# best_rf_regressor = random_search.best_estimator_\n",
    "# best_rf_regressor.fit(X_train_combined, y_train)\n",
    "\n",
    "# y_pred = best_rf_regressor.predict(X_test_combined)\n",
    "\n",
    "\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# rmse = np.sqrt(mse) \n",
    "\n",
    "# print(\"mae:\", mae)\n",
    "# print(\"mse:\", mse)\n",
    "# print(\"rmse:\", rmse)\n",
    "\n",
    "# y_pred_rounded = np.round(y_pred)\n",
    "# accuracy = (y_pred_rounded == y_test).mean()\n",
    "# print(\"ac:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# sample = sample.reset_index(drop=True)\n",
    "\n",
    "# texts = sample['cleaned_text'].tolist()\n",
    "# labels = sample['overall'].astype(int) - 1  \n",
    "\n",
    "# train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# train_texts = pd.Series(train_texts).reset_index(drop=True)\n",
    "# val_texts = pd.Series(val_texts).reset_index(drop=True)\n",
    "# train_labels = pd.Series(train_labels).reset_index(drop=True)\n",
    "# val_labels = pd.Series(val_labels).reset_index(drop=True)\n",
    "\n",
    "# class ReviewDataset(Dataset):\n",
    "#     def __init__(self, texts, labels, tokenizer, max_length):\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts[idx]\n",
    "#         label = self.labels[idx]\n",
    "\n",
    "#         encoding = self.tokenizer.encode_plus(\n",
    "#             text,\n",
    "#             max_length=self.max_length,\n",
    "#             padding='max_length',\n",
    "#             truncation=True,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': encoding['input_ids'].flatten(),\n",
    "#             'attention_mask': encoding['attention_mask'].flatten(),\n",
    "#             'labels': torch.tensor(label, dtype=torch.long)\n",
    "#         }\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)  # 5 для рейтингов от 1 до 5\n",
    "\n",
    "# train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "# val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir='./logs',\n",
    "# )\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     labels = pred.label_ids\n",
    "#     preds = pred.predictions.argmax(-1)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     return {\n",
    "#         'accuracy': acc,\n",
    "#         'f1': f1,\n",
    "#         'precision': precision,\n",
    "#         'recall': recall\n",
    "#     }\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(f\"Evaluation results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение классов в процентах:\n",
      "overall\n",
      "4.0    39.51\n",
      "5.0    29.89\n",
      "3.0    16.90\n",
      "2.0     7.11\n",
      "1.0     6.59\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "class_distribution = sample['overall'].value_counts(normalize=True) * 100\n",
    "print(\"Распределение классов в процентах:\")\n",
    "print(class_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tensor([3.0349, 2.8129, 1.1834, 0.5062, 0.6691])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.array([0, 1, 2, 3, 4])\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=sample['overall'].astype(int) - 1\n",
    ")\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"Class weights:\", class_weights_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
    "\n",
    "# class ReviewDataset(Dataset):\n",
    "#     def __init__(self, texts, labels, tokenizer, max_length):\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts[idx]\n",
    "#         label = self.labels[idx]\n",
    "\n",
    "#         encoding = self.tokenizer.encode_plus(\n",
    "#             text,\n",
    "#             max_length=self.max_length,\n",
    "#             padding='max_length',\n",
    "#             truncation=True,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': encoding['input_ids'].flatten(),\n",
    "#             'attention_mask': encoding['attention_mask'].flatten(),\n",
    "#             'labels': torch.tensor(label, dtype=torch.long)\n",
    "#         }\n",
    "\n",
    "# class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# def custom_loss_fn(outputs, labels):\n",
    "#     loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "#     return loss_fct(outputs, labels)\n",
    "\n",
    "# class DistilBERTWithCustomLoss(DistilBertForSequenceClassification):\n",
    "#     def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "#         outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         logits = outputs.logits\n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             loss = custom_loss_fn(logits, labels)\n",
    "#         return (loss, logits) if loss is not None else logits\n",
    "\n",
    "# model = DistilBERTWithCustomLoss.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
    "\n",
    "# texts = sample['cleaned_text'].tolist()\n",
    "# labels = sample['overall'].astype(int) - 1\n",
    "# train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# train_texts = pd.Series(train_texts).reset_index(drop=True)\n",
    "# val_texts = pd.Series(val_texts).reset_index(drop=True)\n",
    "# train_labels = pd.Series(train_labels).reset_index(drop=True)\n",
    "# val_labels = pd.Series(val_labels).reset_index(drop=True)\n",
    "\n",
    "# train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "# val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir='./logs'\n",
    "# )\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     labels = pred.label_ids\n",
    "#     preds = pred.predictions.argmax(-1)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     return {\n",
    "#         'accuracy': acc,\n",
    "#         'f1': f1,\n",
    "#         'precision': precision,\n",
    "#         'recall': recall\n",
    "#     }\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(f\"Evaluation results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU Name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "texts = sample['cleaned_text'].tolist()\n",
    "labels = sample['overall'].astype(int) - 1\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_texts = pd.Series(train_texts).reset_index(drop=True)\n",
    "val_texts = pd.Series(val_texts).reset_index(drop=True)\n",
    "train_labels = pd.Series(train_labels).reset_index(drop=True)\n",
    "val_labels = pd.Series(val_labels).reset_index(drop=True)\n",
    "\n",
    "train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-5, 2e-5, 3e-5, 5e-5],\n",
    "    'batch_size': [8, 16, 32],\n",
    "    'num_train_epochs': [3, 4, 5],\n",
    "    'weight_decay': [0.0, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "n_iter_search = 10\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "results = []\n",
    "\n",
    "for _ in tqdm(range(n_iter_search)):\n",
    "    learning_rate = random.choice(param_grid['learning_rate'])\n",
    "    batch_size = random.choice(param_grid['batch_size'])\n",
    "    num_train_epochs = random.choice(param_grid['num_train_epochs'])\n",
    "    weight_decay = random.choice(param_grid['weight_decay'])\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",  \n",
    "    save_strategy=\"epoch\",        \n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True   \n",
    ")\n",
    "\n",
    "    \n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    accuracy = eval_results['eval_accuracy']\n",
    "    \n",
    "    results.append({\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'num_train_epochs': num_train_epochs,\n",
    "        'weight_decay': weight_decay,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'num_train_epochs': num_train_epochs,\n",
    "            'weight_decay': weight_decay\n",
    "        }\n",
    "\n",
    "print(\"Best accuracy:\", best_accuracy)\n",
    "print(\"Best parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "texts = sample['cleaned_text'].tolist()\n",
    "labels = sample['overall'].astype(int) - 1\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_texts = pd.Series(train_texts).reset_index(drop=True)\n",
    "val_texts = pd.Series(val_texts).reset_index(drop=True)\n",
    "train_labels = pd.Series(train_labels).reset_index(drop=True)\n",
    "val_labels = pd.Series(val_labels).reset_index(drop=True)\n",
    "\n",
    "train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-5, 2e-5, 3e-5, 5e-5],\n",
    "    'batch_size': [4, 8, 16], \n",
    "    'num_train_epochs': [3, 4, 5],\n",
    "    'weight_decay': [0.0, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "n_iter_search = 10\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "results = []\n",
    "\n",
    "for _ in tqdm(range(n_iter_search)):\n",
    "    learning_rate = random.choice(param_grid['learning_rate'])\n",
    "    batch_size = random.choice(param_grid['batch_size'])\n",
    "    num_train_epochs = random.choice(param_grid['num_train_epochs'])\n",
    "    weight_decay = random.choice(param_grid['weight_decay'])\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=\"./results/checkpoint-5000\")\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    accuracy = eval_results['eval_accuracy']\n",
    "    \n",
    "    results.append({\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'num_train_epochs': num_train_epochs,\n",
    "        'weight_decay': weight_decay,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'num_train_epochs': num_train_epochs,\n",
    "            'weight_decay': weight_decay\n",
    "        }\n",
    "\n",
    "print(\"Best accuracy:\", best_accuracy)\n",
    "print(\"Best parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9881f42cd64c798ae0068f3a25feeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9382, 'grad_norm': 11.984975814819336, 'learning_rate': 5.9e-06, 'epoch': 2.05}\n",
      "{'loss': 0.9501, 'grad_norm': 9.625772476196289, 'learning_rate': 5.8e-06, 'epoch': 2.1}\n",
      "{'loss': 0.9423, 'grad_norm': 38.229515075683594, 'learning_rate': 5.7e-06, 'epoch': 2.15}\n",
      "{'loss': 0.929, 'grad_norm': 13.139717102050781, 'learning_rate': 5.600000000000001e-06, 'epoch': 2.2}\n",
      "{'loss': 1.0011, 'grad_norm': 20.939939498901367, 'learning_rate': 5.500000000000001e-06, 'epoch': 2.25}\n",
      "{'loss': 0.8744, 'grad_norm': 15.448114395141602, 'learning_rate': 5.400000000000001e-06, 'epoch': 2.3}\n",
      "{'loss': 0.911, 'grad_norm': 16.059059143066406, 'learning_rate': 5.300000000000001e-06, 'epoch': 2.35}\n",
      "{'loss': 0.9292, 'grad_norm': 8.150343894958496, 'learning_rate': 5.2e-06, 'epoch': 2.4}\n",
      "{'loss': 0.8781, 'grad_norm': 14.617883682250977, 'learning_rate': 5.1e-06, 'epoch': 2.45}\n",
      "{'loss': 0.8849, 'grad_norm': 21.745532989501953, 'learning_rate': 5e-06, 'epoch': 2.5}\n",
      "{'loss': 0.9396, 'grad_norm': 10.71281909942627, 'learning_rate': 4.9000000000000005e-06, 'epoch': 2.55}\n",
      "{'loss': 0.9367, 'grad_norm': 10.98515796661377, 'learning_rate': 4.800000000000001e-06, 'epoch': 2.6}\n",
      "{'loss': 0.9618, 'grad_norm': 13.427684783935547, 'learning_rate': 4.7e-06, 'epoch': 2.65}\n",
      "{'loss': 0.958, 'grad_norm': 15.874284744262695, 'learning_rate': 4.600000000000001e-06, 'epoch': 2.7}\n",
      "{'loss': 0.9323, 'grad_norm': 19.28169822692871, 'learning_rate': 4.5e-06, 'epoch': 2.75}\n",
      "{'loss': 0.8989, 'grad_norm': 15.677239418029785, 'learning_rate': 4.4e-06, 'epoch': 2.8}\n",
      "{'loss': 0.8919, 'grad_norm': 31.496906280517578, 'learning_rate': 4.3e-06, 'epoch': 2.85}\n",
      "{'loss': 0.9125, 'grad_norm': 9.826878547668457, 'learning_rate': 4.2000000000000004e-06, 'epoch': 2.9}\n",
      "{'loss': 0.9248, 'grad_norm': 19.909130096435547, 'learning_rate': 4.1e-06, 'epoch': 2.95}\n",
      "{'loss': 0.9243, 'grad_norm': 22.353334426879883, 'learning_rate': 4.000000000000001e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93dfecf5f9c8415581b4640c7d938edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.163157343864441, 'eval_accuracy': 0.504, 'eval_f1': 0.48759004282789303, 'eval_precision': 0.5222496841650311, 'eval_recall': 0.504, 'eval_runtime': 74.1314, 'eval_samples_per_second': 26.979, 'eval_steps_per_second': 6.745, 'epoch': 3.0}\n",
      "{'loss': 0.7748, 'grad_norm': 15.175155639648438, 'learning_rate': 3.900000000000001e-06, 'epoch': 3.05}\n",
      "{'loss': 0.8259, 'grad_norm': 10.728374481201172, 'learning_rate': 3.8000000000000005e-06, 'epoch': 3.1}\n",
      "{'loss': 0.8366, 'grad_norm': 15.747048377990723, 'learning_rate': 3.7e-06, 'epoch': 3.15}\n",
      "{'loss': 0.8293, 'grad_norm': 14.31823444366455, 'learning_rate': 3.6000000000000003e-06, 'epoch': 3.2}\n",
      "{'loss': 0.8521, 'grad_norm': 12.997308731079102, 'learning_rate': 3.5e-06, 'epoch': 3.25}\n",
      "{'loss': 0.8114, 'grad_norm': 16.842798233032227, 'learning_rate': 3.4000000000000005e-06, 'epoch': 3.3}\n",
      "{'loss': 0.8566, 'grad_norm': 17.93301773071289, 'learning_rate': 3.3000000000000006e-06, 'epoch': 3.35}\n",
      "{'loss': 0.8231, 'grad_norm': 21.153003692626953, 'learning_rate': 3.2000000000000003e-06, 'epoch': 3.4}\n",
      "{'loss': 0.8446, 'grad_norm': 30.803869247436523, 'learning_rate': 3.1000000000000004e-06, 'epoch': 3.45}\n",
      "{'loss': 0.8382, 'grad_norm': 16.973304748535156, 'learning_rate': 3e-06, 'epoch': 3.5}\n",
      "{'loss': 0.8307, 'grad_norm': 14.024436950683594, 'learning_rate': 2.9e-06, 'epoch': 3.55}\n",
      "{'loss': 0.7556, 'grad_norm': 24.594104766845703, 'learning_rate': 2.8000000000000003e-06, 'epoch': 3.6}\n",
      "{'loss': 0.7922, 'grad_norm': 24.15384292602539, 'learning_rate': 2.7000000000000004e-06, 'epoch': 3.65}\n",
      "{'loss': 0.8552, 'grad_norm': 35.06586456298828, 'learning_rate': 2.6e-06, 'epoch': 3.7}\n",
      "{'loss': 0.7625, 'grad_norm': 11.781449317932129, 'learning_rate': 2.5e-06, 'epoch': 3.75}\n",
      "{'loss': 0.8566, 'grad_norm': 26.33967399597168, 'learning_rate': 2.4000000000000003e-06, 'epoch': 3.8}\n",
      "{'loss': 0.8057, 'grad_norm': 17.49735450744629, 'learning_rate': 2.3000000000000004e-06, 'epoch': 3.85}\n",
      "{'loss': 0.8135, 'grad_norm': 9.141125679016113, 'learning_rate': 2.2e-06, 'epoch': 3.9}\n",
      "{'loss': 0.8542, 'grad_norm': 15.367015838623047, 'learning_rate': 2.1000000000000002e-06, 'epoch': 3.95}\n",
      "{'loss': 0.8196, 'grad_norm': 34.71818161010742, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481b39056e65464fa654b2b091516d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1738511323928833, 'eval_accuracy': 0.518, 'eval_f1': 0.5097164049591331, 'eval_precision': 0.5097679251178411, 'eval_recall': 0.518, 'eval_runtime': 73.4835, 'eval_samples_per_second': 27.217, 'eval_steps_per_second': 6.804, 'epoch': 4.0}\n",
      "{'loss': 0.759, 'grad_norm': 44.20051574707031, 'learning_rate': 1.9000000000000002e-06, 'epoch': 4.05}\n",
      "{'loss': 0.7007, 'grad_norm': 20.3327579498291, 'learning_rate': 1.8000000000000001e-06, 'epoch': 4.1}\n",
      "{'loss': 0.7274, 'grad_norm': 39.0705680847168, 'learning_rate': 1.7000000000000002e-06, 'epoch': 4.15}\n",
      "{'loss': 0.6956, 'grad_norm': 16.035017013549805, 'learning_rate': 1.6000000000000001e-06, 'epoch': 4.2}\n",
      "{'loss': 0.7828, 'grad_norm': 25.091630935668945, 'learning_rate': 1.5e-06, 'epoch': 4.25}\n",
      "{'loss': 0.7676, 'grad_norm': 15.901244163513184, 'learning_rate': 1.4000000000000001e-06, 'epoch': 4.3}\n",
      "{'loss': 0.7574, 'grad_norm': 17.855220794677734, 'learning_rate': 1.3e-06, 'epoch': 4.35}\n",
      "{'loss': 0.6686, 'grad_norm': 18.65996742248535, 'learning_rate': 1.2000000000000002e-06, 'epoch': 4.4}\n",
      "{'loss': 0.7895, 'grad_norm': 20.26844596862793, 'learning_rate': 1.1e-06, 'epoch': 4.45}\n",
      "{'loss': 0.7117, 'grad_norm': 15.364988327026367, 'learning_rate': 1.0000000000000002e-06, 'epoch': 4.5}\n",
      "{'loss': 0.7535, 'grad_norm': 19.34902572631836, 'learning_rate': 9.000000000000001e-07, 'epoch': 4.55}\n",
      "{'loss': 0.7134, 'grad_norm': 25.41085433959961, 'learning_rate': 8.000000000000001e-07, 'epoch': 4.6}\n",
      "{'loss': 0.7531, 'grad_norm': 25.967010498046875, 'learning_rate': 7.000000000000001e-07, 'epoch': 4.65}\n",
      "{'loss': 0.8167, 'grad_norm': 19.99315071105957, 'learning_rate': 6.000000000000001e-07, 'epoch': 4.7}\n",
      "{'loss': 0.7374, 'grad_norm': 13.140756607055664, 'learning_rate': 5.000000000000001e-07, 'epoch': 4.75}\n",
      "{'loss': 0.7529, 'grad_norm': 21.237102508544922, 'learning_rate': 4.0000000000000003e-07, 'epoch': 4.8}\n",
      "{'loss': 0.7654, 'grad_norm': 13.750202178955078, 'learning_rate': 3.0000000000000004e-07, 'epoch': 4.85}\n",
      "{'loss': 0.746, 'grad_norm': 26.970561981201172, 'learning_rate': 2.0000000000000002e-07, 'epoch': 4.9}\n",
      "{'loss': 0.7732, 'grad_norm': 19.917156219482422, 'learning_rate': 1.0000000000000001e-07, 'epoch': 4.95}\n",
      "{'loss': 0.7318, 'grad_norm': 19.435096740722656, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f439f75fe4414191b703aa6eb90734e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2139137983322144, 'eval_accuracy': 0.5105, 'eval_f1': 0.5055354227332404, 'eval_precision': 0.506699732952091, 'eval_recall': 0.5105, 'eval_runtime': 70.6237, 'eval_samples_per_second': 28.319, 'eval_steps_per_second': 7.08, 'epoch': 5.0}\n",
      "{'train_runtime': 2743.0722, 'train_samples_per_second': 14.582, 'train_steps_per_second': 1.823, 'train_loss': 0.4986089179992676, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5000, training_loss=0.4986089179992676, metrics={'train_runtime': 2743.0722, 'train_samples_per_second': 14.582, 'train_steps_per_second': 1.823, 'total_flos': 1324744857600000.0, 'train_loss': 0.4986089179992676, 'epoch': 5.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DistilBertForSequenceClassification\n",
    "\n",
    "learning_rate = 8e-6\n",
    "num_train_epochs = 5  \n",
    "weight_decay = 0.01\n",
    "batch_size = 4  \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./final_model',\n",
    "    evaluation_strategy=\"epoch\",  \n",
    "    save_strategy=\"epoch\",        \n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=\"./results/checkpoint-2000\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
